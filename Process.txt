**Process Management**

Process is nothing but a program in execution

Example -  Work is a program and the work i was doing can be said as a process.

->Programming
->Debugging
->Testing

so on....

A Process has various attributes associated with it. Some of the attributes of a Process are:

Process Id: Every process will be given an id called Process Id to uniquely identify that process from the other processes.
Process state: Each and every process has some states associated with it at a particular instant of time. This is denoted by process state. It can be ready, waiting, running, etc.
CPU scheduling information: Each process is executed by using some process scheduling algorithms like FCSF, Round-Robin, SJF, etc.
I/O information: Each process needs some I/O devices for their execution. So, the information about device allocated and device need is crucial.


Process needs resources to complete its task
When the computer is given with the tasks and the execution of that tasks is a process
The program is loaded into the memory is known as the process
and it as four types 
->Stack- Process Stack contains the temporary Data such as method/function, return address, and local variables
->heap - Dynamically allocated memory 
->Text - General Current activity representated by the value of program counter 

why do we ned new process?

-->To maintain the concurrency, we need another new process and that can be achieved by using fork() system call.

Concurrency - Running Multiple instructions at a time, example - Multiple persons can't draw amount in different places so, that can be done by the concurrency

Process Life Cycle

1	
Start

This is the initial state when a process is first started/created.

2	
Ready

The process is waiting to be assigned to a processor. Ready processes are waiting to have the processor allocated to them by the operating system so that they can run. Process may come into this state after Start state or while running it by but interrupted by the scheduler to assign CPU to some other process.

3	
Running

Once the process has been assigned to a processor by the OS scheduler, the process state is set to running and the processor executes its instructions.

4	
Waiting

Process moves into the waiting state if it needs to wait for a resource, such as waiting for user input, or waiting for a file to become available.

5	
Terminated or Exit

Once the process finishes its execution, or it is terminated by the operating system, it is moved to the terminated state where it waits to be removed from main memory.

?. When the Process gets terminated

--> The Process gets terminated after it's execution or any other interrupt raised by the user

-----------------------------------------------------------
Process control block

This represents a information about a specific process

->The operating system will create a data structure to store the information about the process 
->The process control block is kept in a memory area that is protected from the normal user access

Process Control Block is a data structure that contains information of the process related to it. The process control block is also known as a task control block, entry of the process table, etc.

It is very important for process management as the data structuring for processes is done in terms of the PCB. It also defines the current state of the operating system.

Structure of the Process Control Block
The process control stores many data items that are needed for efficient process management. Some of these data items are explained with the help of the given diagram −

The following are the data items −

Process State
This specifies the process state i.e. new, ready, running, waiting or terminated.

Process Number
This shows the number of the particular process.

Program Counter
This contains the address of the next instruction that needs to be executed in the process.

Registers
This specifies the registers that are used by the process. They may include accumulators, index registers, stack pointers, general purpose registers etc.

List of Open Files
These are the different files that are associated with the process

CPU Scheduling Information
The process priority, pointers to scheduling queues etc. is the CPU scheduling information that is contained in the PCB. This may also include any other scheduling parameters.

Memory Management Information
The memory management information includes the page tables or the segment tables depending on the memory system used. It also contains the value of the base registers, limit registers etc.

I/O Status Information
This information includes the list of I/O devices used by the process, the list of files etc.

Accounting information
The time limits, account numbers, amount of CPU used, process numbers etc. are all a part of the PCB accounting information.

Location of the Process Control Block
The process control block is kept in a memory area that is protected from the normal user access. This is done because it contains important process information. Some of the operating systems place the PCB at the beginning of the kernel stack for the process as it is a safe location.

***Process Control block (PCB) is a data structure that stores information of a process. PCBs are stored in specially reserved memory for the operating system known as kernel space.***
***Process Control Block (PCB) does not contain the boostrap program***
**The Number of process completed per unit time is known as Throughput.**

Degree of Multiprogramming - The Number of processes in Memory.

----------------------------------------------------------

Process Stage - The Condition of the Process States

__Process__Queues___

Process Queue is used to hold the process related process control blocks, When the process make transition from one state to another state the pcb is un-linked and assigned to the other process with the help of process scheduling 

The Operating system manages various types of queues for each of the process states. The PCB related to the process is also stored in the queue of the same state. If the Process is moved from one state to another state then its PCB is also unlinked from the corresponding queue and added to the other state queue in which the transition is made.

1. Job Queue
In starting, all the processes get stored in the job queue. It is maintained in the secondary memory. The long term scheduler (Job scheduler) picks some of the jobs and put them in the primary memory.

2. Ready Queue
Ready queue is maintained in primary memory. The short term scheduler picks the job from the ready queue and dispatch to the CPU for the execution.

3. Waiting Queue
When the process needs some IO operation in order to complete its execution, OS changes the state of the process from running to waiting. The context (PCB) associated with the process gets stored on the waiting queue which will be used by the Processor when the process finishes the IO.


->Process Scheduling 

Comparison among Scheduler

 Long-Term Scheduler	                           Short-Term Scheduler                                    Medium-Term Scheduler

-It is a job scheduler	                        -It is a CPU scheduler	                                -It is a process swapping scheduler.
-Speed is lesser than short term scheduler	    -Speed is fastest among other two	               -Speed is in between both short and long term scheduler.
-It controls the degree of multiprogramming	 -It provides lesser control over degree of multiprogramming	-It reduces the degree of multiprogramming.
-It is almost absent or minimal in time sharing system	-It is also minimal in time sharing system	      -It is a part of Time sharing systems.
-It selects processes from pool                	-It selects those processes which are ready to execute	   -It can re-introduce                            and loads them into memory for execution                                                                    the process into memory and execution can be continued.


Scheduling removes a currently running job from the processor and picks another task to perform. Process Scheduling divides a process into stages: ready, waiting, and running.

There are two ways of scheduling:

1. Non-Preemptive: The resource cannot be withdrawn from a process in non-preemptive until the process has completed execution. Resources are switched when a running process quits and changes to a waiting state.

2. Preemptive: The operating system provides resources to a process for a set period in preemptive scheduling. During the allocation of resources, the process transitions from running to ready or from waiting to ready. This switching happens because the CPU may grant precedence to other processes and substitute the executing process with a higher priority process.

   A Process Scheduler schedules different processes to be assigned to the CPU based on particular scheduling algorithms. There are six popular process scheduling algorithms which we are going to discuss in this chapter −

   - First-Come, First-Served (FCFS) Scheduling
   - Shortest-Job-Next (SJN) Scheduling
   - Priority Scheduling
   - Shortest Remaining Time
   - Round Robin(RR) Scheduling
   - Multiple-Level Queues Scheduling

   These algorithms are either **non-preemptive or preemptive**. Non-preemptive algorithms are designed so that once a process enters the running state, it cannot be preempted until it completes its allotted time, whereas the preemptive scheduling is based on priority where a scheduler may preempt a low priority running process anytime when a high priority process enters into a ready state.

   ## First Come First Serve (FCFS)

   - Jobs are executed on first come, first serve basis.
   - It is a non-preemptive, pre-emptive scheduling algorithm.
   - Easy to understand and implement.
   - Its implementation is based on FIFO queue.
   - Poor in performance as average wait time is high.

   ## Shortest Job Next (SJN)

   - This is also known as **shortest job first**, or SJF

   - This is a non-preemptive, pre-emptive scheduling algorithm.

   - Best approach to minimize waiting time.

   - Easy to implement in Batch systems where required CPU time is known in advance.

   - Impossible to implement in interactive systems where required CPU time is not known.

   - The processer should know in advance how much time process will take.

     ## Priority Based Scheduling

     - Priority scheduling is a non-preemptive algorithm and one of the most common scheduling algorithms in batch systems.

     - Each process is assigned a priority. Process with highest priority is to be executed first and so on.

     - Processes with same priority are executed on first come first served basis.

     - Priority can be decided based on memory requirements, time requirements or any other resource requirement.

       ## Shortest Remaining Time

       - Shortest remaining time (SRT) is the preemptive version of the SJN algorithm.
       - The processor is allocated to the job closest to completion but it can be preempted by a newer ready job with shorter time to completion.
       - Impossible to implement in interactive systems where required CPU time is not known.
       - It is often used in batch environments where short jobs need to give preference.

       ## Round Robin Scheduling

       - Round Robin is the preemptive process scheduling algorithm.
       - Each process is provided a fix time to execute, it is called a **quantum**.
       - Once a process is executed for a given time period, it is preempted and other process executes for a given time period.
       - Context switching is used to save states of preempted processes.

Types of Process Scheduling

1). Long-Term Scheduler

A long-term scheduler determines which system should process programs. It uses CPU Scheduling to select and load programs into memory for execution. It manages the degree of multiprogramming and offers a balanced combination of jobs such as I/O bound and CPU bound.

A steady degree of multiprogramming indicates that the average rate of initialization and the average rate of process departure from the system are equal. Many systems, such as time-sharing operating systems, do not have a long-term scheduler since it is only necessary when a process transitions from new to ready.

Example of a Long Term Scheduler

Consider a long-term scheduler that creates all I/O bound processes. In this case, the CPU will remain idle for most of the time and we will not be utilizing our CPU to its full potential. On the other hand, if a long-term scheduler creates all CPU-bound processes, then the I/O waiting for queue will remain idle for most of the time, the devices will remain unused, and the CPU will remain busy for a long time.

In these scenarios, we will not be utilizing the complete potential of our system. However, if the long term scheduler creates processes that have a perfect balance of both the CPU bound and I/O bound jobs, we will be utilizing the system to its full potential, hence reducing the process time.

2). Medium-Term Scheduler

Medium-term scheduling, which is part of swapping, removes programs from memory. It lowers the amount of multiprogramming and handles the switched-out processes. For enhancing the process mix, swapping is required.

The use of medium term scheduler is to improve multiprogramming by allowing multiple processes to reside in main memory by swapping out processes that are waiting (need I/O) or low priority processes and swapping in other processes that were in ready queue.

When a current process makes an I/O request, it is suspended, which cannot be performed. As a result, the suspended process is transferred to secondary storage to free up memory for other operations. This is known as switching, and the act of swapping is referred to as switched out or rolled out.

For example, in the round-robin process, after a fixed time quantum, the process is again sent to the ready state from the running state. So, these things are done with the help of Medium-Term schedulers.

3). Short-Term Scheduler
A short-term scheduler, often known as a CPU scheduler, boosts system performance based on a predefined set of parameters. This is the process's transition from the ready state to the operating state.

It chooses one of the many processes ready to be executed and assigns the CPU to one of them. It is speedier than long-term schedulers, and it is also known as a dispatcher since it decides which process will be run next.

Short-term scheduling involves selecting one of the processes from the ready queue and scheduling them for execution. This is done by the short-term scheduler. A scheduling algorithm is used to decide which process will be scheduled for execution next by the short-term scheduler.

For example, when a running process is interrupted and may be changed, the short-term scheduler must recalibrate and give the highest-priority job the green light.

(*). Context Switching

A context switching is the mechanism to store and restore the state or context of a CPU in Process Control block so that a process execution can be resumed from the same point at a later time. Using this technique, a context switcher enables multiple processes to share a single CPU. Context switching is an essential part of a multitasking operating system features.

When the scheduler switches the CPU from executing one process to execute another, the state from the current running process is stored into the process control block. After this, the state for the process to run next is loaded from its own PCB and used to set the PC, registers, etc. At that point, the second process can start executing.

Context switches are computationally intensive since register and memory state must be saved and restored. To avoid the amount of context switching time, some hardware systems employ two or more sets of processor registers. When the process is switched, the following information is stored for later use.

Program Counter
Scheduling information
Base and limit register value
Currently used register
Changed State
I/O State information
Accounting information

The need for Context switching
A context switching helps to share a single CPU across all processes to complete its execution and store the system's tasks status. When the process reloads in the system, the execution of the process starts at the same point where there is conflicting.

Following are the reasons that describe the need for context switching in the Operating system.

The switching of one process to another process is not directly in the system. A context switching helps the operating system that switches between the multiple processes to use the CPU's resource to accomplish its tasks and store its context. We can resume the service of the process at the same point later. If we do not store the currently running process's data or context, the stored data may be lost while switching between processes.
If a high priority process falls into the ready queue, the currently running process will be shut down or stopped by a high priority process to complete its tasks in the system.
If any running process requires I/O resources in the system, the current process will be switched by another process to use the CPUs. And when the I/O requirement is met, the old process goes into a ready state to wait for its execution in the CPU. Context switching stores the state of the process to resume its tasks in an operating system. Otherwise, the process needs to restart its execution from the initials level.
If any interrupts occur while running a process in the operating system, the process status is saved as registers using context switching. After resolving the interrupts, the process switches from a wait state to a ready state to resume its execution at the same point later, where the operating system interrupted occurs.
A context switching allows a single CPU to handle multiple process requests simultaneously without the need for any additional processors.

Example of Context Switching
Suppose that multiple processes are stored in a Process Control Block (PCB). One process is running state to execute its task with the use of CPUs. As the process is running, another process arrives in the ready queue, which has a high priority of completing its task using CPU. Here we used context switching that switches the current process with the new process requiring the CPU to finish its tasks. While switching the process, a context switch saves the status of the old process in registers. When the process reloads into the CPU, it starts the execution of the process when the new process stops the old process. If we do not save the state of the process, we have to start its execution at the initial level. In this way, context switching helps the operating system to switch between the processes, store or reload the process when it requires executing its tasks.

Context switching triggers
Following are the three types of context switching triggers as follows.

Interrupts
Multitasking
Kernel/User switch
Interrupts: A CPU requests for the data to read from a disk, and if there are any interrupts, the context switching automatic switches a part of the hardware that requires less time to handle the interrupts.

Multitasking: A context switching is the characteristic of multitasking that allows the process to be switched from the CPU so that another process can be run. When switching the process, the old state is saved to resume the process's execution at the same point in the system.

Kernel/User Switch: It is used in the operating systems when switching between the user mode, and the kernel/user mode is performed.

What is the PCB?
A PCB (Process Control Block) is a data structure used in the operating system to store all data related information to the process. For example, when a process is created in the operating system, updated information of the process, switching information of the process, terminated process in the PCB.

Steps for Context Switching
There are several steps involves in context switching of the processes. The following diagram represents the context switching of two processes, P1 to P2, when an interrupt, I/O needs, or priority-based process occurs in the ready queue of PCB.
As we can see in the diagram, initially, the P1 process is running on the CPU to execute its task, and at the same time, another process, P2, is in the ready state. If an error or interruption has occurred or the process requires input/output, the P1 process switches its state from running to the waiting state. Before changing the state of the process P1, context switching saves the context of the process P1 in the form of registers and the program counter to the PCB1. After that, it loads the state of the P2 process from the ready state of the PCB2 to the running state.

The following steps are taken when switching Process P1 to Process 2:
First, thes context switching needs to save the state of process P1 in the form of the program counter and the registers to the PCB (Program Counter Block), which is in the running state.
Now update PCB1 to process P1 and moves the process to the appropriate queue, such as the ready queue, I/O queue and waiting queue.
After that, another process gets into the running state, or we can select a new process from the ready state, which is to be executed, or the process has a high priority to execute its task.
Now, we have to update the PCB (Process Control Block) for the selected process P2. It includes switching the process state from ready to running state or from another state like blocked, exit, or suspend.
If the CPU already executes process P2, we need to get the status of process P2 to resume its execution at the same time point where the system interrupt occurs.
Similarly, process P2 is switched off from the CPU so that the process P1 can resume execution. P1 process is reloaded from PCB1 to the running state to resume its task at the same point. Otherwise, the information is lost, and when the process is executed again, it starts execution at the initial level.

Alice and Bob have been working hard to learn the concepts of an operating system, and they are here to learn context switching in an operating system.

Professor: Bob, can you explain what is multitasking system?

Bob: Yes, Professor, multiprocessing systems are the type of systems where multiple processes can be executed even if the system contains only one CPU.

Professor: Very good, Bob! But have you ever wondered how multiple processes are executed on the system having a single CPU?

Bob: No, but I am curious to learn that.

Alice: Yes, Professor!

Professor: The magic of executing multiple processes on a single CPU takes place with the help of Context Switching.

When any process is utilizing CPU, and some other process requires the CPU, Context Switching is carried out before passing the control of CPU from the old process to the new process.

Alice: Professor, what does context switching actually do, and why is Context Switching necessary?

Professor: Let's have a look.

Context Switching stores the state of the current process, and the state includes the information about the data that was stored in registers, the value of the program counter, and the stack pointer. Context Switching is necessary because if we directly pass the control of CPU to the new process without saving the state of the old process and later if we want to resume the old process from where it was stopped, we won't be able to do that as we don't know what was the last instruction the old process executed. Context Switching overcomes this problem by storing the state of the process.

Alice: Oh! Now I understand why Context Switching is essential.

Bob: Yes, imagine if Context Switching is not there, and you are taking notes in a word file, and each time you browse something on the web and switch back, you need to rewrite the entire notes. Giggles

Professor: Haha! That's a nice way to put the concept. Now let's see in which situations the context switch takes place.

Advantage of Context Switching
The main advantage of context switching is even if the system contains only one CPU, it gives the user an illusion that the system has multiple CPUs due to which multiple processes are being executed. The context switching is so fast that the user won't even realize that the processes are switched to and fro.
The Disadvantage of Context Switching
Though the context switching time is very less, still during that time, the CPU remains idle and does not do any fruitful work.
Also, due to Context Switching, there is a frequent flush of TLB(Translation Lookaside Buffer) and Cache.
Professor: Let's conclude whatever we have learned so far.

-----------------------------------------------------------------------

Inter-process Communication

- Processes often need to communicate with each other in order to
  accomplish useful tasks. The OS provides several mechanisms to
  enable processes to communicate with each other.

- Shared memory is the simplest way in which two processes can
  communicate with each other. By default, two separate processes have
  two separate memory images, and do not share any memory. (Note: a
  forked child has the same memory image as the parent at creation,
  but any changes made to the child's memory will not be reflected in
  the parent.) Processes wishing to share memory can request the
  kernel for a shared memory segment. Once they get access to the
  shared memory segment, it can be used as regular memory, with the
  property that changes made by one process will be visible to the
  other and vice versa.

- A significant issue with using shared memory is the problem of
  synchronization and race conditions. For example, if several
  processes share a memory segment, it is possible for two processes
  to make a concurrent update, resulting in a wrong value. Therefore,
  appropriate synchronization mechanisms like locking should be used
  when using shared memory.

- Signals are another light-weight way for processes and kernel to
  communicate with each other, and are mainly used to notify processes
  of events. For example, when a user hits Ctrl+C, the OS sends a a
  signal called SIGINT to the running process (as part of handling the
  interrupt generated by the keyboard). When a process receives a
  signal, the normal execution of the process is halted, and a
  separate piece of code called the signal handler is executed by the
  process. A process template comes with default signal handlers for
  all signals, so that every program will not need to have code to
  handle signals. For example, for the Ctrl+C signal, the process will
  terminate by default. However, a program can have its own in-built
  signal handling function, that can be passed to the kernel with the
  "signal" system call. The OS will then invoke this new function when
  the process receives a signal. Signals are not just for
  communication between the OS and processes. One process can use
  signals to communicate with another process also. The "kill" system
  call can be used by a process to send a signal to another process
  whose pid is specified as an argument to the system call.

- Sockets are a mechanism to communicate between two processes on the
  same machine, and even between processes on different
  machines. Network sockets are a standard way for two application
  layer processes running on different machines (e.g., a web client
  and a web server) to exchange data with each other. Similarly, two
  processes on the same machine can use Unix domain sockets to send
  and receive messages between each other. The usage of Unix domain
  sockets is very similar to the usage of network sockets. That said,
  sockets are more widely used for communicating across hosts than
  across processes on the same host.

- Sockets present an interesting case study on blocking
  vs. non-blocking system calls. Some socket system calls (e.g.,
  accept, read) are blocking. For example, when a process reads from a
  socket, the system call blocks until data appears on the socket. As
  a result, while the process is blocked, it cannot handle data coming
  on any other socket.  This limits the number of concurrent
  communications a process can sustain. There are several techniques
  to fix this problem. A process could fork off a new child process
  for every connection it has, so that a child process can be
  dedicated to reading and writing on one connection
  only. Alternately, a socket can be set to be non-blocking, and the
  process can periodically poll the socket to see if data has
  arrived. Finally, system calls such as "select" can be used to get
  notifications from the kernel on when a socket is ready for a read.

- Pipes: a pipe is a half-duplex connection between two file
  descriptors --- data written into one file descriptor can be read
  out through the other. A pair of file descriptors can be bound this
  way using the "pipe" system call. The two ends of the pipe are
  referred to as a read end and a write end. Reading from and writing
  to a pipe can be blocking or non-blocking, depending on how the pipe
  is configured. The data written to a pipe is stored in temporary
  memory by the OS, and is made available to the process that reads
  from it.

- Pipes are anonymous, i.e., there is no way to refer to them outside
  the process. The typical use case of pipes is for a parent process
  to create a pipe, and hand over endpoints to one or more
  children. Alternately, named pipes or FIFOs (First-In-First-Out)
  enable a process to create a pipe with a specified name, so that the
  endpoints of the pipe can be accessed outside the process as well.

- Message passing is another IPC mechanism provided by many
  operating systems. A process can create a message queue (much like a
  mailbox), and another process can send a message to this queue via
  the OS. A message queue is maintained as a linked list inside the
  kernel. Every message has a type, content, and other optional
  features like priority. System calls are available to create a
  message queue, and post and retrieve messages from the queue. As in
  the previous cases, blocking and non-blocking versions of the system
  calls are available. For example, when the message queue is full,
  the writing process can choose to block or return with an error
  message. Similarly, when the message queue is empty, the system call
  to read a message can block or return empty.
  
- NEED OF IPC IN OS
  
- Let us now discuss several reasons for which a process needs to communicate or share data with other processes. Following are few reasons:

  Information Sharing: Several users may need to access the same piece of information (e.g., a shared file). Thus, there needs to be an environment for concurrent access to the shared information
  Computation Speedup: Often a task is split into several sub-tasks to speed up its execution. This also requires related processes to exchange information related to the task
  Modularity: Most of the time applications are built in a modular fashion and divided into separate processes. For instance, the Google Chrome web browser spawns a separate process for each new tab

- Modes of IPC in OS

- There are two modes through which processes can communicate with each other – shared memory and message passing. As the name suggests, the shared memory region shares a shared memory between the processes. On the other hand, the message passing lets processes exchange information through messages. Let’s explore these in detail in the subsequent sections.

- **Inter-process communication (IPC)** is set of interfaces, which is usually programmed in order for the programs to communicate between series of processes. This allows running programs concurrently in an Operating System. These are the methods in IPC:

- ### Shared Memory

  Interprocess communication through the shared memory model **requires communicating processes to establish a shared memory region**. In general, the process that wants to communicate creates the shared memory region in its own address space. Other processes that wish to communicate to this process need to attach their address space to this shared memory segment:

  By default, the operating system prevents processes from accessing other process memory. The shared memory model requires processes to agree to remove this restriction. Besides, as shared memory is established based on the agreement between processes, the processes are also responsible to ensure synchronization so that both processes are not writing to the same location at the same time.

  ### Message Passing

  Although the shared memory model is useful for process communication, it is not always suitable and achievable. For instance, in a distributed computing environment, the processes exchanging data might reside in different computer systems. Thus, it is not straightforward to establish a shared memory region for communication.

  The message passing mechanism provides an alternative means processes for communication. In this mode, **processes interact with each other through messages with assistance from the underlying** 

  In the above diagram two processes A, and B are communicating with each other through message passing. Process A sends a message M to the operating system (kernel). This message is then read by process B.

  In order to successfully exchange messages, there needs to be a communication link between the processes. There are several techniques through which these communication links are established. Following are a bride overview of these mechanisms:

  - Direct Communication: In the mode, each process explicitly specifies the recipient or the sender. For instance, if process A needs to send a message to process B, it can use the primitive ![send(B, message)](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-57f3851b9267f84d41b851f7a1ab1944_l3.svg)
  - Indirect Communication: In this mode, processes can exchange messages through a mailbox. A mailbox is a container that holds the messages. For instance, if X is a mailbox, then process A can send a message to mailbox ![X](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-996ff7036e644e89f8ac379fa58d0cf7_l3.svg) using the primitive ![send(X, message)](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-db4ea05bbeb6a9a0a9b2c1f5225c1a74_l3.svg)
  - Synchronization: This is an extension to direct and indirect communication with an additional option of synchronization. Based on the need. a process can choose to block while sending or receiving messages. Besides, it can also asynchronously communicate without any blocking
  - Buffering: The exchanged messages reside in a temporary queue. These queues can be of zero, bounded, and unbounded capacity

  One classical **example of IPC is the producer-consumer problem**. A producer is an application/process that produces data. A consumer is an application/process that consumes the produced data. The producer and consumer processes can decide any of the mechanisms discussed before to communicate with each other.

- **PIPE** 
Pipes
It is a half-duplex method (or one-way communication) used for IPC between two related processes.
It is like a scenario like filling the water with a tap into a bucket. The filling process is writing into the pipe and the reading process is retrieved from the pipe.
- # Pipes and FIFOs

  [Pipes](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Glossary.html#term-pipe) allow processes to communicate using a unidirectional byte stream, with two ends designated by distinct [file descriptors](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Glossary.html#term-file-descriptor). A common visual analogy for a pipe is a real-world water pipe; water that is poured into one end of the pipe comes out the other end. In the case of IPC, the “water” is the sequence of bytes being sent between the processes; the bytes are written into one end of the pipe and read from the other end. Pipes have several important characteristics that shape their use:

  > - Pipes are *unidirectional*; one end must be designated as the reading end and the other as the writing end. Note that there is no restriction that different processes must read from and write to the pipe; rather, if one process writes to a pipe then immediately reads from it, the process will receive its own message. If two processes need to exchange messages back and forth, they should use two pipes.
  > - Pipes are *order preserving*; all data read from the receiving end of the pipe will match the order in which it was written into the pipe. There is no way to designate some data as higher priority to ensure it is read first.
  > - Pipes have a *limited capacity* and they use [blocking I/O](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Glossary.html#term-blocking-i-o); if a pipe is full, any additional writes to the pipe will block the process until some of the data has been read. As such, there is no concern that the messages will be dropped, but there may be performance delays, as the writing process has no control over when the bytes will be removed from the pipe.
  > - Pipes send data as unstructured *byte streams*. There are no pre-defined characteristics to the data exchanged, such as a predictable message length. The processes using the pipe must agree on a communication protocol and handle errors appropriately (such as if one of the processes terminates the communication early).
  > - Messages that are smaller than the size specified by `PIPE_BUF` are guaranteed to be sent [atomically](https://w3.cs.jmu.edu/kirkpams/OpenCSF/Books/csf/html/Glossary.html#term-atomic). As such, if two processes write to a pipe at the same time, both messages will be written correctly and they will not interfere with each other.

   Basic Pipes

  The simplest form of communication with pipes is to provide parent-child communication using the `pipe()` library function. This function takes an int array of length 2. (Recall that arrays are always passed by reference.) Assuming the kernel is able to create the pipe, the array will contain the file descriptors for the two ends of the pipe. If the pipe creation, the function returns -1.
